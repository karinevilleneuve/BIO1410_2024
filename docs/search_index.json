[["index.html", "BIO1410 - 2024 - Analyses R Chapitre 1 Introduction", " BIO1410 - 2024 - Analyses R Karine Villeneuve Chapitre 1 Introduction Ce petit guide à été rédigé à l’intention des élèves du cours BIO1410 afin de familiariser les étudiants à l’analyse de données de séquençage d’amplicons avec R. Avant de débuter toute analyse, il est nécessaire de comprendre les principales fonctionnalités de R. La permière section Introduction à R Studio et R Markdown présente donc très brièvement comment utiliser R Mardown avec R Studio. Par la suite, la section Tableau de mes ASVs et BLAST comprend un petit script R permettant à l’élève d’identifier les 10 genres bactériens les plus abondants dans ses échantillons récoltés sous sa langue ainsi que sur la paume de sa mains. Ce script permet aussi de générer un tableau de données en format comma seperated value (csv) que l’étudiant devra utiliser pour obtenir les séquences d’acides nucléiques à rechercher sur la base de données du National Center for Biotechnology Information (NCBI) avec leur outil Basic Local Alignment Search Tool (BLAST). La section Traitement des fastqs présente ensuite comment les données brutes du séquençage d’amplicon du gène ARNs 16S ont été traitées dans R afin d’obtenir une matrice de l’abondance relative des variants de séquence d’amplicon (ASVs) par échantillon. Finalement, la section Statistiques présente l’ensemble des analyses statistiques réalisées avec R afin de comparer le microbiome des deux régions échantillonner. "],["introduction-à-r-studio-et-r-markdown.html", "Chapitre 2 Introduction à R Studio et R Markdown 2.1 Pourquoi utiliser R Markdown 2.2 Utliser R Markdowm", " Chapitre 2 Introduction à R Studio et R Markdown 2.1 Pourquoi utiliser R Markdown Plusieurs préfèrent rédiger leurs scripts dans des fichiers de type R Markdown plutôt que R Script pour diverses raions. Je recommande personnellement d’utiliser R Markdown pour les raisons suivantes : Ce type de fichier permet de facilement annoter son script entre les sections de codes qui sont comprises dans un bloc (“chunk” en anglais). Un script bien annoter permet non seulement au scripteur de s’y retrouver facilement mais aussi de partager son code à ses collègues ou même publiquement. En effet, il est de plus en plus commun de retrouver dans les articles scientifiques un lien vers les scripts générés et uilisés par les auteurs de l’article afin d’analyser leurs données. Mais aussi, les blocs de codes permettent d’exécuter seulement certaines sections de code à la fois, ce qui ultimement permet de mofidier puis exécuter seulement ces sections de code sans avoir à re-exécuter l’entièreté du script. 2.2 Utliser R Markdowm La première étape consiste donc à créer un nouveau document de type R Mardown. Pour cela, ouvrez RStudio, puis cliquez sur Fichier. Dans le menu déroulant sélectionner Nouveau Fichier puis R Markdown.... Dans la nouvelle fenêtre vous pouvez donner le titre que vous voulez à votre nouveau document. Par défault le nouveau document affiche une petite introduction ainsi que des exemples tel que sur l’image ci-dessous : Ces informations ne sont pas pertinentes et vous pouvez supprimer l’ensemble du texte sous l’entête (l’entête correspond à la section délimitée par les trois tirets ---). 2.2.1 Bloc de codes Dans R markdown, les lignes de codes à exécuter doivent être comprises dans un bloc de code. Le texte non compris dans un bloc n’est donc pas considéré comme du code, ce qui permet d’annoter minutieusement votre script entre les blocs afin de vous y retrouver facilement. Un bloc de code R doit toujours débuter avec les caractères suivants : ```{r} et se terminer avec les caractères suivant : ```. Un bloc de code ressemble donc à ceci : ```{r} ``` Un bloc de code peut être inséré avec l’une des façons suivantes : le raccourcit clavier : Ctrl + Alt + I tapper manuellement les caractères délimitants (```{r} ``` ) l’onglet Code puis Insert chunk le bouton vert avec le petit c et signe de plus en haut à droite. Un fois votre code rédigé dans le bloc, vous pouvez exécuter l’entièreté du code contenu dans ce block en appuyant sur le bouton vert en haut à droite du code ( ▶ ). Il est possible d’insérer des blocs de code de différents languages de programmation tels que Bash et Python, il suffit de remplacer le r entre les accolades par le nom du programme utilisé. Plusieurs autres options peuvent être appliqués sur les blocs, pour plus d’informations je vous recommande de consulter la documentation disponible sur internet. 2.2.2 Définir le répertoire de travail par défault Généralement, avant de commencer à analyser des données, il faut indiquer à R où se trouve les données sur notre ordinateur. Il existe plusieurs manières de procéder, dont notamment, spécifier au début du script un répertoire/dossier de travail par défault. Pour ce faire, une suffit d’utiliser la commande setwd() comme présenté sur l’exemple ci-dessous. Le texte entre les guillemets de l’exemple doit simplement être modifié pour le vrai chemin vers votre répertoire/dossier de travail. Une fois le repertoire de travail définit par défaut de la sorte il n’est pas nécessaire d’inclure le chemin vers les fichiers que l’on veut importer tant qu’il se trouve dans le répertoire spécifié. Cela s’applique aussi à la sauvegarde de tableaux, figures ou autres. ```{r} setwd(&quot;/chemin/vers/le/répertoire/&quot;) ``` 2.2.3 Importer des données Pour importer un fichier en format comma seperated value (csv) dans R on utilise la commande read.csv avec les arguments suivants : file : Spécifier le nom du fichier header : Indiquer si la première ligne de notre tableau correspond au nom des colonnes (valeurs possible TRUE/FALSE) row.names : Indiquer quelle colone de notre tableau correspond au nom des rangés sep : Indiquer quel caractère doit servir de séparateur pour les colonnes check.name : Ne pas systématiquement remplacer le trait d’union par un point (valeurs possible TRUE/FALSE) ```{r} df = read.csv(file = &quot;nom_fichier.csv&quot;, header = TRUE, row.names = 1, sep = &quot;,&quot;, check.names = FALSE) ``` Pour un tableau en format tab delimited on peut utiliser la fonction read.table et spécifier \\t comme séparateur. "],["tableau-de-mes-asvs-et-blast.html", "Chapitre 3 Tableau de mes ASVs et BLAST 3.1 Scientifiques 3.2 Obtenir le tableau d’abondance de mes ASVs dans Excel 3.3 BLAST", " Chapitre 3 Tableau de mes ASVs et BLAST Vous avez vue en classe une figure représentant l’abondance relative des principaux genres bactériens présents sous la langue ainsi que sur la paume de la main dominante de chacun des scientifiques de votre classe. Vous aimeriez maintenant générer une figure similaire à celle présentée en classe mais comprenant uniquement vos échantillons. Pour ce faire, vos démonstrateurs ont rédigé ce petit script R. Il vous suffit de suivre les étapes suivantes : Télécharger le tableau de données bio1410_2024.csv et l’enregistrer dans le dossier Documents de l’ordinateur que vous utilisez. Si vous utilisez votre ordinateur personnel plutôt que celui de l’école vos démonstrateurs pourrons vous aider à spécifier le dossier le travail. Créer un nouveau document de type R markdown. Insérer deux blocs de code R et y copier/coller les blocs de code ci-dessous. À la première ligne du code, copier/coller le nom de votre scientifique à partir de la liste des scienfitiques disponible après les blocs des code. Attention, assurez-vous de ne pas avoir d’espace entre les guillemets et le nom. Exécuter les commandes en appuyant sur le bouton vert en haut à droite du bloc de code ( ▶ ). Bloc de code 1 : Installation des packages/libraires install.packages(&quot;randomcoloR&quot;) install.packages(&quot;ggplot2&quot;) library(ggplot2) Bloc de code 2 : Script pour obtenir vos données # Définir le nom de votre scientifique ma_scientifique = &quot;Abigail A. Salyers&quot; # Importer le tableau de données tableau = read.table(file = &quot;bio1410_2024.csv&quot;, sep=&quot;,&quot;, header=TRUE, check.names=FALSE) # Définir le rang taxonomique d&#39;intérêt taxa_rank = &quot;Genus&quot; # Extraire vos échantillons mes_échantillons = subset(tableau, Scientifique == ma_scientifique &amp; Abundance &gt; 0) # Afin de visualiser uniquementles 10 taxons les plus abondants on utilise ce qu&#39;on appelle une *for loop* ou boucle en français empt_df = list() # On génère une liste vide que l&#39;on viendra populer avec les résultats de la foor loop i = 0 # Ensuite on définit un premier itérateur (de type numérique) qui compte le nombre de loop effectué # Début de la for loop for (région_échantillonnée in unique(mes_échantillons$Region)){ # Cette séquence est le début de la loop où l&#39;itérateur de gauche (région) i = i + 1 # représente chaque item de l&#39;itérateur de droite (mes_échantillons$Région) sample = subset(mes_échantillons, Region == région_échantillonnée) # Générer une nouvelle table de données basée sur la variable définit par l&#39;itérateur total_abundance = aggregate(sample$Abundance, by = list(taxa_rank = sample[[taxa_rank]]), FUN = sum) # Additionner ensemble les ASVs du même genres top = head(total_abundance[order(total_abundance$x, decreasing = T),], n = 5) # filtrer en order croissant pour identifer les 10 plus abondants others_df = sample[!sample[[taxa_rank]] %in% top$taxa_rank,] # identifier les autres genres ne faisant pas partie des 10 plus abonants others_list = others_df[[taxa_rank]] # extraire leur nom du tableau de données dans une liste sample[sample[[taxa_rank]] %in% others_list,][[taxa_rank]] = &quot;Autres&quot; # Renommer leur genre pour Autres empt_df[[i]] = sample # Enregistrer ce nouveau tableau } df = do.call(&quot;rbind&quot;,empt_df) # Combiner les deux tables de données en une seule # Maintenant que nous avons notre tableau de données avec seulement les 10 taxons les plus abondants sous la langue et la main # nous pouvons commencer à travailler sur le graphique. On commence par générer une liste de couleurs associées à chacun des taxons n = nrow(data.frame(unique(df$Genus))) # On calcule le nobre de genre unique que nous avons palette = randomcoloR::distinctColorPalette(n) # On génère une nouvelle palette my_scale = ggplot2::scale_fill_manual(name = as.character(taxa_rank), values = palette, na.translate = FALSE, drop = TRUE, limits = force) # On assigne une couleur à chacun de nos genres uniques # Nous pouvons finalement générer le graphique graphique = ggplot2::ggplot(df, aes(x = Region, weight = Abundance, fill = .data[[taxa_rank]])) + facet_grid(~ Region, scales = &quot;free&quot;) + geom_bar() + labs(y = &quot;Abondance relative (%)&quot;) + scale_y_continuous(expand = c(0,0)) + theme_classic() + theme(text = element_text(size = 12), strip.text.x = element_blank(), plot.title = element_text(hjust = 0.5, margin = margin(0, 0, 50, 0))) + guides(fill = guide_legend(title = &quot;Genre&quot;, title.position = &quot;top&quot;)) + my_scale graphique # visualiser le graphique produit # Enregistrer en format coma separeted value (csv) votre tableau d&#39;abondance write.csv(df, &quot;df.csv&quot;, quote=FALSE) 3.1 Scientifiques A - J K - V Abigail A. Salyers Kalpana Chawla Alice Augusta Ball Khatijah Mohammad Yousoff Ana Roque de Duprey Leone Norwood Farrell Angelina Fanny Hesse Lise Meitner Augusta Ada Byron Lovelace Margaret Elaine Heafield Hamilton Chien-Shiung Wu Marie Maynard Daly Clark Creola Katherine Coleman Johnson Marjory Stephenson Dorothy Hodgkin Mary Golda Ross Dorothy Jean Johnson Vaughan Mary Jackson Elizabeth Bugie Gregory Nettie Stevens Elizabeth Garrett Anderson Rachel Carson Emmanuelle Marie Charpentier Rosalind Elsie Franklin Emmy Klieneberger-Nobel Roseli Ocampo-Friedmann Farah Alibay Ruth Ella Moore Floy Agnes Naranjo Stroud Lee Sally Kristen Ride Frances Joan Estelle Wagner Vera Florence Cooper Rubin Gladys Mae Brown West Jane Goodale Jessie Isabelle Price Johanna Westerdijk June Dalziel Hart Almeida 3.2 Obtenir le tableau d’abondance de mes ASVs dans Excel Le fichier df.csv aura été enregistré dans votre working directory, soit le dossier Documents. Ouvrir le fichier df.csv dans Excel Dans Excel, sélectionner l’ensemble d’une colonne (par ex. en cliquant sur l’en-tête ‘A’). Aller dans l’onglet Données, cliquer sur Convertir. À « Choisisser le type de fichier qui décrit le mieux vos données », sélectionner Délimité. Cliquer sur Suivant. Sélectionner le séparateur Virgule. Cliquer sur Terminer. Dans l’onglet Données, cliquer sur Trier. Sélection Trier par Région, ordre de A à Z. Cliquer sur Ajouter un niveau, sélectionner Puis par abundance, ordre de Z à A. Copier la séquence de nucléotides de l’ASV le plus abondant de la paume (colonne M; Séquence). Faire l’exercice BLAST ci-bas avec l’ASV le plus abondant de la paume, puis après, avec l’ASV le plus abondant sous la langue. 3.3 BLAST Analyse de l’identité taxonomique de séquences microbiennes du gène d’ARNr 16S Méthod Se rendre sur le logiciel en ligne BLAST du NCBI Choisir l’analyse Nucleotide BLAST (BLASTn) Copier votre séquence dans la boîte de dialogue Choisir la banque de données appropriée à laquelle comparer votre séquence Utiliser la collection de nucléotides Exclure les organismes environnementaux et non cultivés Choisir l’algorithme de BLAST désiré (essayer à la fois les algorithmesBLASTn etMegaBLAST Lancer l’analyse (bouton BLAST). Après lecture et analyse du fichier sortant effectuer un imprime-écran incluant les trois premiers résultats de l’analyse (trois premiers taxons identifiés). Vous devrez inclure cette image dans l’annexe de votre travail. Réalisez un tableau à inclure dans le corps du texte comprenant les colonnes suivantes; Per. Ident. Query cover E.value Vous pouvez aussi consulter pour votre plaisir personnel : Le rapport de taxonomie (taxonomy report) L’arbre des distances entre les meilleurs résultats (distance tree of results) "],["traitement-des-fastqs.html", "Chapitre 4 Traitement des fastqs 4.1 Pré-analyse 4.2 Inspecter la qualité, filtrer et rogner 4.3 Retirer les erreurs de nucléotides 4.4 Gnérer la matrice d’abondance d’ASVs par échantillon 4.5 Classification taxonomique 4.6 Obtenir les séquences d’acides nucléiques (ADN) 4.7 Sauvegarder les résultats en format comma seperated value (csv) 4.8 Raréfier 4.9 Sauvegarder les résultats en format comma seperated value (csv)", " Chapitre 4 Traitement des fastqs DADA2 est un pipeline bio-informatique qui comprend une série d’étapes permettant de filtrer les séquences brutes obtenues par le séquençage d’amplicons. Ultimement, le pipeline permet d’obtenir une matrice d’abondance d’ASVs en fonction des échantillons ainsi que la classification taxonomique des séquences associées aux différents ASVs. DADA2 a deux particularités qui le distingue des autres pipeline courament utilisés. D’une part, il procède la à modélisation de l’erreur dûe au séquençage ce qui est censé permettre des distinguer les séquences mutantes, des séquences érronées. D’autre part, contrairement à d’autres pipelines comme QIIME ou Mothur, DADA2 ne regroupe pas les séquences similaires à 97% en Unités Taxonomiques Opérationelles (OTUs). Ses ASVs ne subissent pas de regroupement si les séqences ne sont pas identiques à 100%. 4.1 Pré-analyse Importer les librairies que nous utilisons pour traiter les données library(tidyr) library(dplyr) library(dada2) library(phyloseq) library(DECIPHER) Identifier les échantillons présents dans le dossier de travail et extraire le nom de ces deniers sans les artefacts ajoutés lors du séquençage. # Créer une variable qui définit le lien vers le dossier contenant tous les fichiers en format fastq path = &quot;~/BIO1410/2023/raw_fastqs&quot; # Identifier tous les fichiers fastq du brin Forward fnFs = sort(list.files(path, pattern=&quot;_R1_001.fastq&quot;, full.names = TRUE)) # Identifier tous les fichiers fastq du brin Reverse fnRs = sort(list.files(path, pattern=&quot;_R2_001.fastq&quot;, full.names = TRUE)) # Extraire le nom de l&#39;échantillon à partir du nom du fichier en assumant # que le format du nom de l&#39;échantillon est le suivant : NOM-ÉCHANTILLON_XXX.fastq.gz sample.names = sapply(strsplit(basename(fnFs), &quot;_&quot;), `[`, 1) # Appliquer ce nom sans les artefacts du séquençage à nos variables names(fnFs) = sample.names names(fnRs) = sample.names 4.2 Inspecter la qualité, filtrer et rogner Inspecter la qualités des séquences Afin d’inspecter la qualité des séquences nous utilisons la commande plotQualityProfile. Cette commande représente graphiquement le Q score (axe des Y) associé à chaque nucléotide (axe des X). La médianne est en vert, et les quartiles en orange pointillé. Le Q score est une mesure qui nous renseigne sur la précision du séquençage (voir tableau ci-dessous). Q score Précision 10 90 % 20 99 % 30 99.9 % 40 99.9 % plotQualityProfile(fnFs, aggregate = TRUE) plotQualityProfile(fnRs, aggregate = TRUE) L’analyse de ces graphiques nous permet donc de choisir les paramètres de filtrage et de rognage de la prochaine étape. La qualité des séquences du brin Forward est généralement de meilleur qualité que celle des séquences du brin Reverse. L’intégralité du brin Forward (250 nucléotides) est d’assez bonnes qualité pour le conserver entier alors que pour le brin Reverse nous supprimerons les nucléotides après la position 240. En observant les graphiques générés, vous pouvesz constater que le début des séquences est aussi de moins bonne qualité et comprend les amorces que nous devons retirer. Filtrer et rogner les séquences Tout d’abord nous créons un dossier filtered ainsi que les objets filtFs filtRs pour stoquer les séquences filtrées. filtFs = file.path(path, &quot;filtered&quot;, paste0(sample.names, &quot;_F_filt.fastq.gz&quot;)) filtRs = file.path(path, &quot;filtered&quot;, paste0(sample.names, &quot;_R_filt.fastq.gz&quot;)) names(filtFs) = sample.names names(filtRs) = sample.names Nous utilisons la fonction filterAndTrim afin de filtrer et rogner les séquences. truncQ : définit un l’indice Q score minimale. A la première instance d’un score de qualité inférieur ou égal à truncQ, la séquence est tronquée. truncLen : définit à quelle longueur les séquences vont être tronquées. Les séquences plus courtes que la longueur sont éliminées. trimLeft : définit la longueur que l’on coupe du côté 5’ des séquences. Cela permet d’enlever les amorces si elles n’ont pas été préalablement retiré. 799F: AACMGGATTAGATACCCKG = 19 nucléotides / 1115R: AGGGTTGCGCTCGTTG = 19 nucléotides maxEE : définit le nombre maximum d’erreurs attendues autorisées dans une lecture. Ce filtre se base sur l’indice Q score. Plus on augmente le chiffre, moins on est strict. # Filtrer les séquences out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = c(19,16), truncLen=c(250,240), maxN=0, maxEE=2, truncQ=2,rm.phix=TRUE, compress=TRUE, multithread=TRUE) 4.3 Retirer les erreurs de nucléotides Cet étape consiste à estimer le taux d’erreur de séquençage. Son but est de permettre de différencier les séquences mutantes et les séquences érronées.Le modèle d’erreur est calculé en alternant l’estimation des taux d’erreur et l’inférence de la composition de l’échantillon jusqu’à ce qu’ils convergent vers une solution cohérente. errF = learnErrors(filtFs, multithread=TRUE, randomize=TRUE) errR = learnErrors(filtRs, multithread=TRUE, randomize=TRUE) Nous pouvons maintenant appliquer l’algorithme entraîné sur nos séquences afin de retirer les nulcéotides erronés dadaFs = dada(filtFs, err = errF, pool = &quot;pseudo&quot;, multithread=TRUE) dadaRs = dada(filtRs, err = errR, pool = &quot;pseudo&quot;, multithread=TRUE) 4.4 Gnérer la matrice d’abondance d’ASVs par échantillon Tout l’intérêt du séquençage en paire réside dans le but de fusionner les deux brins afin d’accroitre notre confiance en leur fiabilité. La fusion permet également d’obtenir des amplicons plus long. mergers = mergePairs(dadaFs, filtFs, dadaRs, filtRs) Nous avons enfin notre matrice d’abondance d’ASVs que nous allons stoquer dans l’objet seqtab. # Construire la table d&#39;ASV avec les séquences combinées seqtab = makeSequenceTable(mergers) L’étape de retrait des chimère vise à éliminer toutes les séquences non biologiques qui sont produites lorsqu’une séquence n’est pas complètement amplifié lors de la PCR et qu’elle s’hybride avec l’amorce lors du prochain cycle. seqtab.nochim = removeBimeraDenovo(seqtab, method = &quot;consensus&quot;, multithread = TRUE) # Comparer la dimension de la tables d&#39;ASV avec et sans chimières dim(seqtab.nochim) sum(seqtab.nochim)/sum(seqtab) 4.5 Classification taxonomique Grâce à l’implémentation de la méthode de classification naïve bayésienne, la fonction assignTaxonomy prend en entrée l’ensemble de séquences à classer ainsi qu’un ensemble de séquences de référence avec une taxonomie connue. Nous utilisons la base de données d’amplicons de Silva. taxa = assignTaxonomy(seqtab.nochim, &quot;~/16S_Project/silva_nr99_v138.1_train_set.fa.gz&quot;, multithread=TRUE, tryRC=TRUE) Comme nous avons plusieurs ASVs non classifié à différents rangs taxonomique, nous allons venir copier le rang taxonomique supérieur suivant du mot “Unclassified” aux ASVs non-classifiés. taxid = data.frame(t(taxa)) taxid[] = lapply(taxid, as.character) taxa2 = tidyr::fill(taxid, colnames(taxid),.direction = &quot;down&quot;) taxa2 = sapply(taxa2, function(x){paste0(&quot;Unclassified_&quot;, x)}) taxid[is.na(taxid)] = taxa2[is.na(taxid)] taxid = t(taxid) # Retirer les ASVs n&#39;appartenant pas au domaine des Bactéries taxid = subset(as.data.frame(taxid), Kingdom ==&quot;Bacteria&quot;) seqtab.nochim = seqtab.nochim[,colnames(seqtab.nochim) %in% rownames(taxid)] 4.6 Obtenir les séquences d’acides nucléiques (ADN) Nous voulons conserver la séquences d’ADN correspondant à chacun des ASVs afin de pouvoir ultimement utiliser cette séquences avec la base de données BLAST de NCBI. # Importer le tableau de données des métadonnées meta = read.table(&quot;/home/kvilleneuve/16S_Project/BIO1410/metadata.csv&quot;,sep=&quot;,&quot;, row.names=1, header=TRUE) # Combiner la matrice d&#39;aboncance, le tableau de métadonnées et la table de taxonomique dans un object de type phyloseq ps = phyloseq(otu_table(t(seqtab.nochim), taxa_are_rows=TRUE), tax_table(as.matrix(taxid)), sample_data(meta)) # On dna = Biostrings::DNAStringSet(taxa_names(ps)) names(dna) = taxa_names(ps) ps1 = merge_phyloseq(ps, dna) taxa_names(ps1) = paste0(&quot;ASV&quot;, seq(ntaxa(ps1))) 4.7 Sauvegarder les résultats en format comma seperated value (csv) # La matrice d&#39;anbondance d&#39;ASV par échantillon write.csv(seqtab.nochim, file = &quot;~/BIO1410/raw_asv.csv&quot;) # Le tableau de classification taxonomique des ASVs write.csv(taxid, file = &quot;~/BIO1410/raw_sequences.csv&quot;) # Les séquences d&#39;ADN associées à chacun des ASVs ps1 %&gt;% refseq() %&gt;% Biostrings::writeXStringSet(&quot;~/BIO1410/raw_refseq.fna&quot;, append=FALSE, compress=FALSE, compression_level=NA, format=&quot;fasta&quot;) 4.8 Raréfier Retirer les taxons peu abondant Une des raisons pour retirer les taxons peu abondant est de limiter les temps passé à analyser des taxons rarement vus. Cela s’avère généralement une bonne pratique afin de filtrer le “bruit de fond” inutile (taxons qui ne sont en fait que des artefacts du processus de collecte de données). Nous élminons ici les taxons dont l’abondance relative est inférieur à 0.005 % tel que recommandé par Bokulich et al., 2013 # Define threshold for low abundant taxa minTotRelAbun = 5e-5 # Get total sum for each taxa x = taxa_sums(ps) # Identify taxa with a total sum greater than the defined threshold keepTaxa = (x / sum(x)) &gt; minTotRelAbun # Filter out from the phyloseq object any taxa not identified in the keepTaxa object prunedSet = prune_taxa(keepTaxa, ps) # View how many taxa were removed by sample loss_taxa=data.frame(sample_sums(prunedSet), sample_sums(ps), (sample_sums(prunedSet)-sample_sums(ps))) Nous raréfions ensuite au premier nombre de séquence au-dessus de 1000. Dans note cas cela correspond à 1118. # Get dataframe of sequences per sample sample_size = as.data.frame(sample_sums(prunedSet)) # Filter for the lowest number above 1000 rare_value = sample_size[which.max((sample_size[,1] &gt;= 1000)/sample_size[,1]),] # Rarefy to value identified as rare_value ps_rare = rarefy_even_depth(prunedSet, rare_value, rngseed = 112, replace = FALSE, trimOTUs = TRUE, verbose = TRUE) # Confirm rarefaction as a sanity check sample_sums(ps_rare) 4.9 Sauvegarder les résultats en format comma seperated value (csv) write.csv(as.data.frame(as(tax_table(ps_rare), &quot;matrix&quot;)), file = &quot;~/rarefied_taxa.csv&quot;) write.csv(as.data.frame(as(otu_table(ps_rare), &quot;matrix&quot;)),file = &quot;~/rarefied_asv.csv&quot;) "],["statistiques.html", "Chapitre 5 Statistiques 5.1 Thème pour les graphiques 5.2 Indice de Shannon 5.3 Ordination (PCoA) 5.4 LEfSe 5.5 Abondances relatives", " Chapitre 5 Statistiques Importer les libraires que nous devons utiliser library(vegan) library(tidyverse) library(devtools) library(forcats) # To reorder our factors (function &quot;fct_relevel&quot;) library(dplyr) # Dataframe manipulation (notably function &quot;pull&quot;) library(tidyr) # Dataframe manipulation (function &quot;separate&quot;) library(phyloseq) # Very pratical library for the analysis of amplicon data library(randomcoloR) # Generate sets of random colors library(ggplot2) # Generate plots library(stringr) # Makes working with strings as easy as possible (function &quot;str_replace&quot;) library(ggtext) # Allows the use of markdown text (used to make names italic) library(ggpubr) 5.1 Thème pour les graphiques Je génère mon propre thème pour les différents graphiques à générer dans lequel je spécifie par exemple la taille du texte, la taille des lignes des axes, etc… text_size = 12 custom_theme = function(){ theme_classic() %+replace% theme( #text elements plot.title = element_text(size = text_size), #set font size plot.subtitle = element_text(size = text_size), #font size plot.caption = element_text(size = text_size), #right align axis.title = element_text(size = text_size), #font size axis.text = element_text(size = text_size), #font size axis.text.x = element_text(size = text_size), axis.text.y = element_text(size = text_size), legend.text = element_text(size = text_size), legend.title = element_text(size = text_size, hjust=0), strip.text = element_text(size = text_size), strip.background = element_blank(), panel.border = element_rect(colour = &quot;black&quot;, fill = NA, linewidth = 1), panel.grid.minor = element_blank(), panel.grid.major = element_line(linewidth = 0.25, linetype = &#39;solid&#39;, colour = &quot;grey&quot;) ) } Importer les fichiers produit par le pipeline DADA2 (incluant la raréfaction) asv = read.table(file = &quot;~/rarefied_asv.csv&quot;, sep=&quot;,&quot;, row.names=1, header=TRUE, check.names=FALSE) taxa = read.table(file = &quot;~/rarefied_taxa.csv&quot;, sep=&quot;,&quot;, row.names=1, header=TRUE) meta = read.table(file = &quot;~/metadata.csv&quot;, sep=&quot;,&quot;, row.names=1, header=TRUE) # Insérer le symbole &quot;\\n&quot; (signifie saut de line) afin que le texte s&#39;affiche sur deux lignes dans la légende de notre figure meta$Region = gsub(&quot;Paume de la main dominante&quot;,&quot;Paume de la \\nmain dominante&quot;, meta$Region) # Combiner les tableaux dans un objet de type phyloseq ps = phyloseq(otu_table(asv, taxa_are_rows = TRUE), tax_table(as.matrix(taxa)), sample_data(meta)) 5.2 Indice de Shannon Calculer l’indice de diversité par échantillon shannon = cbind(estimate_richness(ps, measures = &#39;shannon&#39;), sample_data(ps)) Maintenant que nous avons les valeurs de diversité (indice de Shannon) nous aimerions comparer ces valeurs en fonction de la région échantilloné (langue et mains). Pour ce faire, nous pouvons utiliser un test de t de student, mais nous devons avant nous assurer que nos données respectent les postulats de ce test (distribution normale et variance similaire) # Est ce que les données ont une distibution gausienne (normale) ? valeur_shapiro=list() # créer une liste vide # Réaliser le test de shapiro pour chacune des régions for (region in unique(shannon$Region)){ sample = subset(shannon, Region == region) ok = shapiro.test(sample$Shannon) valeur_shapiro = append(valeur_shapiro,ok$p.value) } data.frame(valeur_shapiro) # Pour nos deux régions (langue et mains) la distribution des valeurs de Shannon suit une distribution normale # Est ce que la variance est similaire ? # Nous pouvons facilement tester la vairance avec la fonction var.test var.test(Shannon ~ Region, shannon, alternative = &quot;two.sided&quot;) t.test(Shannon ~ Region, data = shannon) Nos données respectent les postulats du test de t de Student, nous pouvons donc procéder à l’analyse. p = ggplot(shannon, aes(x = Region, y = Shannon, fill = Region)) + custom_theme() + geom_boxplot(alpha = 0.3, outlier.shape = NA) + geom_jitter(aes(color = Region), size = 3, ) + scale_fill_manual(values = c(&quot;cornflowerblue&quot;,&quot;palevioletred&quot;)) + scale_color_manual(values = c(&quot;cornflowerblue&quot;,&quot;palevioletred&quot;)) + labs(x = &quot;Région&quot;, y = expression(paste(&quot;Indice de diversité de Shannon (&quot;, italic(&quot;H&#39;&quot;), &quot;)&quot;))) + scale_y_continuous(limits = c(0,4), expand = c(0,0)) + annotate(geom =&quot;text&quot;, x = 1.5, y = 3.8, label = expression(paste(italic(&quot;p&quot;),&quot; &lt; .001&quot;)), size = 5) 5.3 Ordination (PCoA) # Transformer les données d&#39;abondance relative avec l&#39;indice d&#39;Hellinger asv_hellinger = decostand(asv, method = &quot;hellinger&quot;) # On combine les tableaux de donnnées dans un object phyloseq ps = phyloseq(otu_table(asv_hellinger, taxa_are_rows = TRUE), tax_table(as.matrix(taxa)), sample_data(meta)) # Calculer l&#39;indice de distance de Bray-Curtis entre chaque échantillon dist = vegdist(t(asv_hellinger), method = &quot;bray&quot;) # Générer l&#39;ordination avec les fonction R de bases. PCOA = cmdscale(dist, eig=TRUE, add=TRUE) # On extrait les coordonnées des points position = PCOA$points # Changer le nom des colonnes colnames(position) = c(&quot;Axe.1&quot;, &quot;Axe.2&quot;) # get percentage of variation explained by each axis percent_explained = 100*PCOA$eig/sum(PCOA$eig) # reduce number of digits (arrondir) reduced_percent = format(round(percent_explained[1:2], digits = 2), nsmall = 1, trim = TRUE) # Generate pretty labels for plot pretty_labs = c(glue(&quot;Axe 1 ({reduced_percent[1]}%)&quot;), glue(&quot;Axe 2 ({reduced_percent[2]}%)&quot;)) # combine PCOA results with metadata df = merge(position, meta, by = 0) Générer le graphique plot = ggplot(df, aes(x=Axe.1, y=Axe.2, color=Region)) + custom_theme() + geom_point(size = 3) + labs(x = pretty_labs[1], y = pretty_labs[2], color = &quot;Région&quot;) + scale_y_continuous(limits = c(-0.50,0.50), expand = c(0,0)) + scale_x_continuous(limits = c(-0.5,0.5), expand = c(0,0)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, linewidth = 0.2) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, linewidth = 0.2) + scale_color_manual(values = c(&quot;cornflowerblue&quot;,&quot;palevioletred&quot;)) PERMANOVA et betadisper # Calculer la dispersion (distance au centroïde) dispr = vegan::betadisper(dist, group = meta$Region) boxplot(dispr, main = &quot;&quot;, xlab = &quot;&quot;) # ANOVA sur la dispersion (p = 0.001) permutest(dispr) # ANOVA sur la distance en fonction des région (p = 0.001) adns = adonis2(dist ~ meta$Region) 5.4 LEfSe # Définir le rang taxonomique d&#39;intérêt taxa = &quot;Genus&quot; # On combine les tableaux de données dans un object phyloseq ps = phyloseq(otu_table(asv, taxa_are_rows=TRUE), tax_table(as.matrix(taxa)), sample_data(meta)) # Lancer l&#39;analyse LEfSe out_lefse = run_lefse(ps, group = &quot;Region&quot;,taxa_rank = taxa) # Extraire les résultats sous forme de tableau df_lefse = marker_table(out_lefse) %&gt;% data.frame() # Keeping only the 9 taxa with the highest LDA score in each group number_of_taxa = 5 lda_out = list() i = 0 for (each_region in unique(df_lefse$enrich_group)){ i = i + 1 region = subset(df_lefse, enrich_group == each_region) top_taxa=head(region[order(region$ef_lda, decreasing= T),], n=number_of_taxa) lda_out[[i]] = top_taxa } df = do.call(&quot;rbind&quot;, lda_out) # Multiplier par -1 les score LDA pour la région sous la langue df$ef_lda = with(df, ifelse(enrich_group == &quot;Sous la langue&quot;, -1 * df$ef_lda, 1*df$ef_lda)) Générer le graphique p = ggplot(df, aes(x = ef_lda, y = reorder(feature, ef_lda), fill = enrich_group)) + geom_bar(stat = &quot;identity&quot;, width = 0.7, size = 0.5) + custom_theme() + facet_grid(rows = vars(enrich_group), scales = &quot;free_y&quot;, space = &quot;free_y&quot;) + theme(legend.position = &quot;none&quot;, strip.text.y = element_text(angle =)) + scale_x_continuous(limits=c(-6,6), expand = c(0,0)) + labs(x = &quot;Score LDA&quot;, y = &quot;Genre&quot;) + scale_fill_manual(values=c(&quot;cornflowerblue&quot;,&quot;palevioletred&quot;)) + geom_vline(xintercept = 0, linewidth = 0.25, colour=&quot;black&quot;) 5.5 Abondances relatives # Defining the number of most abundant taxa to keep number_of_taxa = 3 ps_rel_abund=transform_sample_counts(ps, function(x) x/sum(x)) ps_glom = tax_glom(ps_rel_abund, taxrank = taxa_rank) melted_df = psmelt(ps_glom) # Create an empty list that we will populated with the unique taxa of each sample list_of_all_taxonomic_rank= list() i = 0 # Beginning of the for loop for (each_sample in unique(melted_df$Sample)){ i = i + 1 sample = subset(melted_df, Sample == each_sample) # Create a new dataframe from the iterator (sample). total_abundance = aggregate(sample$Abundance, by = list(taxa_rank = sample[[taxa_rank]]), FUN = sum) # Combine together the same taxa and sum the abundances top = head(total_abundance[order(total_abundance$x, decreasing= T),], n = number_of_taxa) # Sort by abundance and keep only the X number of taxa defined by variable number_of_taxa others_df = sample[!sample[[taxa_rank]] %in% top$taxa_rank,] # Extract in a new dataframe all taxa that are not present in the dataframe `top` others_list = pull(others_df, taxa_rank) # Create a list by pulling all the values from the column corresponding to the taxa_rank into a list sample[sample[[taxa_rank]]%in% others_list,][[taxa_rank]] = &quot;Others&quot; # In the dataframe `sample` rename all the taxa from the list `others_list` as `Others` list_of_all_taxonomic_rank[[i]] = sample #save this dataframe in our list } df = do.call(&quot;rbind&quot;,list_of_all_taxonomic_rank) # combine all the dataframe from the list into one dataframe unique_taxon = data.frame(unique(df[[taxa_rank]])) # create dataframe with the unique names of taxa name = colnames(unique_taxon) # extract the name of the column in order to rename the column with the following line names(unique_taxon)[names(unique_taxon) == name] = as.character(taxa_rank) # Rename the column to the taxa rank defined earlier # get the total number of unique most abundant taxa n = nrow(unique_taxon) # generate a set of X unique colors corresponding to the number of unique taxa palette = distinctColorPalette(n) unique_taxon[[taxa_rank]] = factor(unique_taxon[[taxa_rank]]) names(palette) = levels(unique_taxon[[taxa_rank]]) # assign gray to category &quot;Others&quot;. The same nomenclature can be use to manually change certain colors. palette[[&quot;Others&quot;]] = &quot;#E1E1E1&quot; # recreate palette with markdown to italicize name and remove the underscore after Unclassified all_names = data.frame(names(palette)) names_markdown = all_names %&gt;% mutate(names.palette. = str_replace(names.palette., &quot;(.*)&quot;,&quot;*\\\\1*&quot;), # Adding asterisk at beginning and end of every taxa names.palette. = str_replace(names.palette., &quot;\\\\*Unclassified_(.*)\\\\*&quot;,&quot;Unclassified *\\\\1*&quot;), # Removing the asterisk for words that don&#39;t need to be italicize (Unclassified and Others) names.palette. = str_replace(names.palette., &quot;\\\\*Others\\\\*&quot;, &quot;Others&quot;)) list_names=as.vector(names_markdown$names.palette.) # Replace names of object names(palette) = c(list_names) # Making the same modification to the taxa name from the legend to the taxa names in the dataframe df[[taxa_rank]] = str_replace(df[[taxa_rank]], &quot;(.*)&quot;,&quot;*\\\\1*&quot;) df[[taxa_rank]] = str_replace(df[[taxa_rank]], &quot;\\\\*Unclassified_(.*)\\\\*&quot;,&quot;Unclassified *\\\\1*&quot;) df[[taxa_rank]] = str_replace(df[[taxa_rank]], &quot;\\\\*Others\\\\*&quot;, &quot;Others&quot;) # Ordering the legend in alphabetical order legend_raw = unique(df[[taxa_rank]]) #Extract legend as text ordered_legend = sort(legend_raw) # order alphabetically reordered_legend = fct_relevel(ordered_legend, &quot;Others&quot;) # move &quot;Others&quot; to the beginning final_legend = levels(reordered_legend) # Extract the levels in a new object my_scale = scale_fill_manual(name = as.character(taxa_rank), breaks = paste(final_legend), values = palette, na.translate=FALSE, drop=TRUE, limits = force) p = ggplot(df, aes(x = Scientifque, weight = Abundance, fill = fct_reorder(.data[[taxa_rank]],Abundance,.desc=FALSE))) + # .data is very important to force the evaluation of the input variables (taxonomic_rank) geom_bar() + coord_flip() + labs(y =&#39;Abondance relative&#39;, x=&quot;Scientifque&quot;) + theme_classic() + facet_wrap(~ Region, ncol=2, strip.position=&quot;top&quot;,scales=&#39;free_x&#39;) + theme(text = element_text(size = text_size), panel.spacing = unit(2, &quot;lines&quot;), plot.title = element_text(hjust =0.5, size=text_size), axis.title=element_text(size=text_size), axis.text.x = element_text(size=text_size, hjust=0.6), axis.text.y = element_text(size=text_size, vjust=0.5), legend.position = &quot;bottom&quot;, legend.title = element_text(size = text_size), legend.text = element_markdown(size = 12), legend.key.size = unit(0.5, &#39;cm&#39;), legend.margin = margin(), # pre-emptively set zero margins strip.background = element_blank(), strip.text = element_text(size = text_size)) + # remove facet_grid box background my_scale + # Load our color palette scale_y_continuous(limits = c(0, 1),breaks = seq(0, 1, by = 0.5), expand = c(0,0), labels = c(&quot;0&quot;, &quot;0.5&quot;, &quot;1&quot;)) + # Remove the white space # Adjusting the legend, notably the number of rows and position guides(fill = guide_legend(nrow = 8, title = &quot;Genre&quot;, title.position = &quot;top&quot;, title.hjust = 0.5, reverse=FALSE)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
